""" Implementing DCGAN in Pytorch """
from torch.nn import ConvTranspose2d
# use batchnorm in both generator and discriminator
from torch.nn import BatchNorm2d
# replace any pooling layers with Strided Conv (in Discriminator), and Fractional-Strided Conv (Generator)
from torch.nn import Conv2d
from torch.nn import Linear
# use ReLU activation in generator for all layers except output, which uses Tanh
from torch.nn import ReLU
from torch.nn import Tanh
# use LeakyReLU activation in the discriminator for all the layers 
from torch.nn import LeakyReLU

from torch.nn import Sigmoid
from torch import flatten
from torch import nn 



class Generator(nn.Module):
    """ DCGAN Generator Class to random noise into an image
    @para:
    - inputDim: The input size of the noise vector passed through the generator.
    - outputDim: The output height and width of the image generated by the generator
    - outputChannels:The number of channels of the output image. Since we are using the MNIST dataset, 
        the image will be in grayscale. Hence itâ€™ll have a single channel.
    """
    def __init__(self, inputDim = 100, outputDim = 512, outputChannels = 1, debug = False) -> None:
        super(Generator, self).__init__()
        self.DEBUG = debug

        # first set of CONVT -> RELU -> BN
        self.ct1 = ConvTranspose2d(in_channels=inputDim, out_channels= 128, kernel_size= 4, stride= 2, 
                                    padding= 0, bias= False)
        self.relu1 = ReLU()
        self.batchNorm1 = BatchNorm2d(128)

        # 2nd set of CONVT -> RELU -> BN
        self.ct2 = ConvTranspose2d(in_channels=128, out_channels= 64, kernel_size= 3, stride=2, 
                                    padding= 1, bias= False)
        self.relu2 = ReLU()
        self.batchNorm2 = BatchNorm2d(64)

        # last set of CONVT => RELU => BN
        self.ct3 = ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=4, stride=2, 
                                    padding=1, bias=False)
        self.relu3 = ReLU()
        self.batchNorm3 = BatchNorm2d(32)

        # apply another upsample and transposed convolution, but
        # this time output the TANH activation
        self.ct4 = ConvTranspose2d(in_channels=32, out_channels=outputChannels, kernel_size=4, stride=2,
                                    padding=1, bias=False)
        self.tanh = Tanh()


    def forward(self, x):
        # pass the input through the first set of CONVT -> RELU -> BN layers
        x = self.ct1(x)
        x = self.relu1(x)
        x = self.batchNorm1(x)

        # pass the output from previous layer through the second set of CONVT -> RELU -> BN layers set
        x = self.ct2(x)
        x = self.relu2(x)
        x = self.batchNorm2(x)

        # pass the output from previous layer through the third set of CONVT -> RELU -> BN layers set

        x = self.ct3(x)
        x = self.relu3(x)
        x = self.batchNorm3(x)

        # pass the output from previous layer through the last set of CONVT -> RELU -> BN layers set
        x = self.ct4(x)
        x = self.tanh(x)

        # return the output
        return x


class Discriminator(nn.Module): 
    """ DCGAN Discriminator Class, this takes the image and output a probability of the image being real or fake
    @para:
    - depth: determinies the number of channels in the input image. 
    - alpha: The value of alpha used in the leaky ReLU activation function.
    """
    def __init__(self, depth, alpha= 0.2, debug = False) -> None:
        super(Discriminator, self).__init__()
        self.DEBUG = debug

        # first set of CONV -> RELU
        self.conv1 = Conv2d(in_channels= depth, out_channels= 32, kernel_size= 4, stride= 2, padding= 1)
        self.leakyRelu1 = LeakyReLU(alpha, inplace= True)

        # 2nd set of CONV -> RELU
        self.conv2 = Conv2d(in_channels=32, out_channels= 64, kernel_size= 4, stride= 2, padding= 1)
        self.leakyRelu2 = LeakyReLU(alpha, inplace= True)

        # first (and only) set of FC -> RELU 
        self.fc1 = Linear(in_features=3136, out_features= 512)
        self.leakyRelu3 = LeakyReLU(alpha, inplace= True)

        # sigmoid layer to output a probability
        self.fc2 = Linear(in_features=512, out_features= 1)
        self.sigmoid = Sigmoid()   


    def forward(self, x):
        # pass the input through the first set of CONV -> RELU layers
        x = self.conv1(x)
        x = self.leakyRelu1(x)

        # pass the output from previous layer through the second set of CONV -> RELU layers 
        x = self.conv2(x)
        x = self.leakyRelu2(x)
        if self.DEBUG: print("x shape after CONV2  {}".format(x.shape))

        # flatten the output from the previous layer and pass it
		# through our first (and only) set of FC => RELU layers
        x = flatten(x, 1)
        if self.DEBUG:  print("x shape after flatten(x,1)  {}".format(x.shape))
        x = self.fc1(x)

        if self.DEBUG:  print("x shape after self.fc1(x)  {}".format(x.shape))

        x = self.leakyRelu3(x)

        # pass the output from previous layer through the sigmoid layer
        x = self.fc2(x)
        x = self.sigmoid(x)

        # return the output
        return x

    






        



                                
